{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YKnUjumbquh8zIQcKXziRkcfzZM_Z_NV",
      "authorship_tag": "ABX9TyN2yIOVUGQfoLgslkMZ2Uq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostHumble/EN2KAB/blob/main/train_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tokenizers transformers"
      ],
      "metadata": {
        "id": "3L72CYrsEbeK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import os\n",
        "import pandas as pd\n",
        "# Choose the Kaggle API token JSON file that you downloaded\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "8N1U3OsmVlZv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d sifalklioui/wiki-kabyle\n",
        "!unzip wiki-kabyle.zip -d data"
      ],
      "metadata": {
        "id": "NyDfNi4cckNr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = {'unk_token':\"[UNK]\",\n",
        "                  'cls_token ':\"[CLS]\",\n",
        "                  'sep_token':\"[SEP]\",\n",
        "                  'pad_token':\"[PAD]\",\n",
        "                  'mask_token':\"[MASK]\",\n",
        "                  'bos_token':\"[BOS]\"}\n",
        "\n",
        "path = \"/content/data\"\n",
        "files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".txt\")]\n",
        "\n",
        "# This loads an empty instance of the tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# This splits the text into words using the whitspace\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
        "trainer = BpeTrainer(vocab_size=35000, show_progress=True,special_tokens=list(special_tokens.values()))\n",
        "tokenizer.train(files, trainer)\n",
        "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))\n",
        "# Load trained tokenizer for future use\n",
        "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,**special_tokens)"
      ],
      "metadata": {
        "id": "53XYv6zbK-ZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4e628c-f3e2-4339-8aed-b85275bda61f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained vocab size: 35000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data to try the tokinzer\n",
        "df = pd.read_csv('/content/drive/MyDrive/eng2kab.tsv',sep='\\t',names=['id1','en','id2','kab'], header=None).drop(columns=['id1','id2'])\n",
        "# Test\n",
        "fast_tokenizer(df.kab[0],max_length=6,padding='max_length')"
      ],
      "metadata": {
        "id": "oo5itOVFMRo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef98bfad-4c04-4c05-eee4-f5de2b8d4706"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1462, 3101, 6, 3, 3, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}