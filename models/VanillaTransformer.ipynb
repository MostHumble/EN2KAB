{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1a130f14e7f043358916cc067cd297a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28aa98261b6947d9a97858dcff50ff4d","IPY_MODEL_7d2a2d2f64034c21b1d3c7ecc7ba06f7","IPY_MODEL_4bed7ad51aab48d1b9803923ba1aa042"],"layout":"IPY_MODEL_380394d7c571473dbe9d258612987f5d"}},"28aa98261b6947d9a97858dcff50ff4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8064793281bc4eff8d506682f2b1f80b","placeholder":"​","style":"IPY_MODEL_4dda68733df8475bbf4294976181c799","value":"Downloading (…)okenizer_config.json: 100%"}},"7d2a2d2f64034c21b1d3c7ecc7ba06f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a58a16faf4e44ee28c783217b723d871","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e69c52fe88244839ce1d70ededd401a","value":28}},"4bed7ad51aab48d1b9803923ba1aa042":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db7e7b07eb4c46cea94e01574bdd5e08","placeholder":"​","style":"IPY_MODEL_81ac5e57b0ba429fa741b7b320664927","value":" 28.0/28.0 [00:00&lt;00:00, 1.40kB/s]"}},"380394d7c571473dbe9d258612987f5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8064793281bc4eff8d506682f2b1f80b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dda68733df8475bbf4294976181c799":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a58a16faf4e44ee28c783217b723d871":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e69c52fe88244839ce1d70ededd401a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db7e7b07eb4c46cea94e01574bdd5e08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81ac5e57b0ba429fa741b7b320664927":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32d468f9221e4514b133f493a1dc030f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b4e8c740e214363b26b0509762ff130","IPY_MODEL_17078612a6e14a93883325f96feabe55","IPY_MODEL_0629816b65b74c4f8cecf213b2e77c3b"],"layout":"IPY_MODEL_dc677cf184f547ea92f6297d949bd82a"}},"8b4e8c740e214363b26b0509762ff130":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32be718e944942fe839c3264f1e8ed38","placeholder":"​","style":"IPY_MODEL_d6f62776211f49dfa32c6842729daf7b","value":"Downloading (…)lve/main/config.json: 100%"}},"17078612a6e14a93883325f96feabe55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a779eb728cb4a6a8e989ec8c6d577f0","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1c96ea47a8c458b9f980373a0b3df98","value":570}},"0629816b65b74c4f8cecf213b2e77c3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b14ea970f514a9bbb6838faf69c81ab","placeholder":"​","style":"IPY_MODEL_5e3d9bdb866b482ba1dd6636985e62de","value":" 570/570 [00:00&lt;00:00, 27.9kB/s]"}},"dc677cf184f547ea92f6297d949bd82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32be718e944942fe839c3264f1e8ed38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f62776211f49dfa32c6842729daf7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a779eb728cb4a6a8e989ec8c6d577f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c96ea47a8c458b9f980373a0b3df98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b14ea970f514a9bbb6838faf69c81ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3d9bdb866b482ba1dd6636985e62de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33f923d8da7b48569d1a4664cdd61df8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0c89b8f705243e980a3c9525ec420eb","IPY_MODEL_429f32080efb47d1acbf6f9fc2a1141c","IPY_MODEL_34956745f3824600968f181aca0e08d9"],"layout":"IPY_MODEL_d6ffa4759e714bc4a5aa918119a8e498"}},"e0c89b8f705243e980a3c9525ec420eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_166bc6e9565a478e8ac7dd94f660e439","placeholder":"​","style":"IPY_MODEL_e1a87ab4fdd04bcb8303acb921d38d9c","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"429f32080efb47d1acbf6f9fc2a1141c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c433b7cb134e438dbf3f2f4d97dbcb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac714bc71d4249509387746467cc1ba8","value":231508}},"34956745f3824600968f181aca0e08d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49cd6e4478fa4ac7bbf8855d28d4cd67","placeholder":"​","style":"IPY_MODEL_303c1610be9e469d9680de67dcd32ea2","value":" 232k/232k [00:00&lt;00:00, 3.56MB/s]"}},"d6ffa4759e714bc4a5aa918119a8e498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"166bc6e9565a478e8ac7dd94f660e439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1a87ab4fdd04bcb8303acb921d38d9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4c433b7cb134e438dbf3f2f4d97dbcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac714bc71d4249509387746467cc1ba8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49cd6e4478fa4ac7bbf8855d28d4cd67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"303c1610be9e469d9680de67dcd32ea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"deb29c66f35141a0929327573a91ed0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8c9bc95c4ee4fe78457dec83de9d0ce","IPY_MODEL_3490fcabdbfd473f9cd4795d82bff38e","IPY_MODEL_658b56eedc3a4f3b868c105ed739ad7f"],"layout":"IPY_MODEL_09bbc5b394ea48e5b932d95eb871a47f"}},"b8c9bc95c4ee4fe78457dec83de9d0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bacf5321e774317b39e5ac5dba3928d","placeholder":"​","style":"IPY_MODEL_a8ed3621bd3e42dd93b6f52db18bd7d5","value":"Downloading (…)/main/tokenizer.json: 100%"}},"3490fcabdbfd473f9cd4795d82bff38e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c491b5082c149e68c0a3c4f04466e61","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ba46641bd0495192292aaa1788f896","value":466062}},"658b56eedc3a4f3b868c105ed739ad7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee4009f42284792aa1818d63d01c7cc","placeholder":"​","style":"IPY_MODEL_fdfd80902cc344d4b3c15c5b2f1253da","value":" 466k/466k [00:00&lt;00:00, 7.95MB/s]"}},"09bbc5b394ea48e5b932d95eb871a47f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bacf5321e774317b39e5ac5dba3928d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8ed3621bd3e42dd93b6f52db18bd7d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c491b5082c149e68c0a3c4f04466e61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ba46641bd0495192292aaa1788f896":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aee4009f42284792aa1818d63d01c7cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdfd80902cc344d4b3c15c5b2f1253da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc167b07edb640a9be332a8474b5dc52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3daa4eb779f84ef4bed84d9b2d310b66","IPY_MODEL_1b799d15042344bf865374b4634466e1","IPY_MODEL_bd7ac327bb7e4ba5b45374027530c253"],"layout":"IPY_MODEL_45a193e0e1834e60b7fd4365a4c71fad"}},"3daa4eb779f84ef4bed84d9b2d310b66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d9c103f98ab47f6b058d84e9a0dc530","placeholder":"​","style":"IPY_MODEL_88eff466c6054b2b91e38059c7a14d9f","value":"Downloading (…)okenizer_config.json: 100%"}},"1b799d15042344bf865374b4634466e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4089022912854d78858c2559bf7e7943","max":1601,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e3fe4a317194dcdaa8f79f9df8a49af","value":1601}},"bd7ac327bb7e4ba5b45374027530c253":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5d7bc6fe6ba4e5eb1b8eec5f133d908","placeholder":"​","style":"IPY_MODEL_3ba7f4ec5f604684a854d0fafb29c5e7","value":" 1.60k/1.60k [00:00&lt;00:00, 105kB/s]"}},"45a193e0e1834e60b7fd4365a4c71fad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d9c103f98ab47f6b058d84e9a0dc530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88eff466c6054b2b91e38059c7a14d9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4089022912854d78858c2559bf7e7943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e3fe4a317194dcdaa8f79f9df8a49af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5d7bc6fe6ba4e5eb1b8eec5f133d908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba7f4ec5f604684a854d0fafb29c5e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20cfeb09a8c34e5191204329979d3dcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_272927580cff4509a04cec9af2021f58","IPY_MODEL_852f648e082740eb8cbe29aea839be0e","IPY_MODEL_e875a71d27bf493e995353f84cab2af9"],"layout":"IPY_MODEL_98ba872008024a51af8449b30fa410f7"}},"272927580cff4509a04cec9af2021f58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fde3adf95de3469c88f42ddc1413e0d0","placeholder":"​","style":"IPY_MODEL_08dbefdef6b04113b1c03e46d3393110","value":"Downloading (…)/main/tokenizer.json: 100%"}},"852f648e082740eb8cbe29aea839be0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2cd216725040ec959948e83de908d3","max":1359692,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a823ad2373e4383a6836c1f083543c8","value":1359692}},"e875a71d27bf493e995353f84cab2af9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1b4e9ca4a954797b3922a71634a83d9","placeholder":"​","style":"IPY_MODEL_f7d2698d47264aeba67ceab77f9e8f2a","value":" 1.36M/1.36M [00:00&lt;00:00, 14.7MB/s]"}},"98ba872008024a51af8449b30fa410f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fde3adf95de3469c88f42ddc1413e0d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08dbefdef6b04113b1c03e46d3393110":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d2cd216725040ec959948e83de908d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a823ad2373e4383a6836c1f083543c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1b4e9ca4a954797b3922a71634a83d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7d2698d47264aeba67ceab77f9e8f2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6813eda08fdc4a9fb4decc2ec0b33f17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_671026bfa79c44768915cf4503714b2d","IPY_MODEL_1c98901d34784175b93a0185e439b779","IPY_MODEL_65e40755c23d45d9a633f36517e0eea2"],"layout":"IPY_MODEL_73eb29f4f36b49718724f3689cc6521c"}},"671026bfa79c44768915cf4503714b2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99a8485125e24c24bf978e989951d6af","placeholder":"​","style":"IPY_MODEL_b7297e2dc10043aeb500e1e7cea77fcf","value":"Downloading (…)cial_tokens_map.json: 100%"}},"1c98901d34784175b93a0185e439b779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_203761c8e0e74d349192b01a129373b1","max":173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_838124f6ecaa4e9e8b5e0b6555249e9c","value":173}},"65e40755c23d45d9a633f36517e0eea2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be7c54368cd94f74b53884cdc48d5070","placeholder":"​","style":"IPY_MODEL_8e3b0854c61b40dc9db1c6f16f9ffbb2","value":" 173/173 [00:00&lt;00:00, 13.0kB/s]"}},"73eb29f4f36b49718724f3689cc6521c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99a8485125e24c24bf978e989951d6af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7297e2dc10043aeb500e1e7cea77fcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"203761c8e0e74d349192b01a129373b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"838124f6ecaa4e9e8b5e0b6555249e9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be7c54368cd94f74b53884cdc48d5070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3b0854c61b40dc9db1c6f16f9ffbb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install tokenizers transformers datasets lightning-bolts\n!pip install wandb -qU","metadata":{"id":"zE_qPYnwb2w5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\nimport os\nimport pandas as pd\nfrom tokenizers import Tokenizer\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers.processors import TemplateProcessing\nfrom transformers import AutoTokenizer\nimport matplotlib.pyplot as plt\nfrom timeit import default_timer as timer\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom dataclasses import dataclass,asdict\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nimport math\nfrom pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm \nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nimport yaml\nimport random, torch, numpy as np\n\nuser_secrets = UserSecretsClient()\nwandb.login(key=user_secrets.get_secret(\"wbtok\"))\n# Choose the Kaggle API token JSON file that you downloaded\n#files.upload()","metadata":{"id":"lRslFWK0b4ND","execution":{"iopub.status.busy":"2023-10-08T17:41:40.833637Z","iopub.execute_input":"2023-10-08T17:41:40.834497Z","iopub.status.idle":"2023-10-08T17:41:51.790744Z","shell.execute_reply.started":"2023-10-08T17:41:40.834461Z","shell.execute_reply":"2023-10-08T17:41:51.789782Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n/opt/conda/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n/opt/conda/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n  self.nce_loss = AmdimNCELoss(tclip)\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msifalklioui\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Choose the Kaggle API token JSON file that you downloaded\n%%capture\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n!kaggle datasets download -d sifalklioui/wiki-kabyle\n!unzip wiki-kabyle.zip -d data","metadata":{"id":"wK0lh4sH-tN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Monolingual data\npath = \"/kaggle/input/en2kab/eng2kab.tsv\"\n#files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".txt\")]\n# Kabyle English pairs\ndf = pd.read_csv(path,sep='\\t',names=['id1','en','id2','kab'], header=None).drop(columns=['id1','id2'])","metadata":{"id":"65nY8ubScYci","execution":{"iopub.status.busy":"2023-10-08T17:41:51.792654Z","iopub.execute_input":"2023-10-08T17:41:51.793629Z","iopub.status.idle":"2023-10-08T17:41:55.843733Z","shell.execute_reply.started":"2023-10-08T17:41:51.793592Z","shell.execute_reply":"2023-10-08T17:41:55.842751Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def enforce_reproducibility(use_seed=None):\n    seed = use_seed if use_seed is not None else random.randint(1, 1000000)\n    print(f\"Using seed: {seed}\")\n\n    random.seed(seed)    # python RNG\n    np.random.seed(seed) # numpy RNG\n\n    # pytorch RNGs\n    torch.manual_seed(seed)          # cpu + cuda\n    torch.cuda.manual_seed_all(seed) # multi-gpu - can be called without gpus\n    if use_seed: # slower speed! https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark     = False\n\n    return seed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\nspecial_tokens = {'unk_token':\"[UNK]\",\n                  'cls_token':\"[CLS]\",\n                  'eos_token': '[EOS]',\n                  'sep_token':\"[SEP]\",\n                  'pad_token':\"[PAD]\",\n                  'mask_token':\"[MASK]\",\n                  'bos_token':\"[BOS]\"}\n# Load tokenizers\nsource_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", **special_tokens)\ntarget_tokenizer = PreTrainedTokenizerFast.from_pretrained('Sifal/EN2KAB',token='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')\n\ndef addPreprocessing(tokenizer):\n      tokenizer._tokenizer.post_processor = TemplateProcessing(\n          single=tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n          special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id), (tokenizer.bos_token, tokenizer.bos_token_id)])\n\naddPreprocessing(source_tokenizer)\naddPreprocessing(target_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T17:41:55.844897Z","iopub.execute_input":"2023-10-08T17:41:55.845232Z","iopub.status.idle":"2023-10-08T17:41:56.339025Z","shell.execute_reply.started":"2023-10-08T17:41:55.845197Z","shell.execute_reply":"2023-10-08T17:41:56.338021Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"## from dataclasses import dataclass\nseed_ = enforce_reproducibility()\n@dataclass\nclass Config:\n    seed: int = seed_\n    data_folder: str = \"/kaggle/input/en2kab/\"\n    output_dir: str = './logs'\n    src_max_length: int = 32\n    tgt_max_length: int = 32\n    add_special_tokens: bool = True\n    truncation: bool = True\n    return_tensors: str = 'pt'\n    padding: str = \"max_length\"\n    emb_size: int = 512\n    source_vocab_size: int = 30524  # Initialize to 0 or provide the actual value\n    target_vocab_size: int = 30000  # Initialize to 0 or provide the actual value\n    num_encoder_layers: int = 8\n    num_decoder_layers: int = 8\n    nhead: int = 8\n    ffn_hid_dim: int = 512\n    train_batch_size: int = 256\n    eval_batch_size: int = 256\n    learning_rate: float = 2e-4\n    scheduler: str = 'LinearWarmupCosineAnnealingLR'\n    num_train_epochs: int = 30\n    warmup_epochs: int = 10\n    label_smoothing : float = 0.0\n\nrun_num = 7\nconfig = Config()\n\nwandb.init(\n      # Set the project where this run will be logged\n      project=\"EN2KAB\", \n      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n      name=f\"VanillaTransoformer_{run_num}\", \n      # Track hyperparameters and run metadata\n      config=asdict(config))","metadata":{"id":"BRHqzzd2NnT7","execution":{"iopub.status.busy":"2023-10-08T17:44:43.117283Z","iopub.execute_input":"2023-10-08T17:44:43.117622Z","iopub.status.idle":"2023-10-08T17:45:17.985094Z","shell.execute_reply.started":"2023-10-08T17:44:43.117594Z","shell.execute_reply":"2023-10-08T17:45:17.983932Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:vkm9bajx) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">VanillaTransoformer_7</strong> at: <a href='https://wandb.ai/sifalklioui/EN2KAB/runs/vkm9bajx' target=\"_blank\">https://wandb.ai/sifalklioui/EN2KAB/runs/vkm9bajx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231008_174156-vkm9bajx/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:vkm9bajx). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231008_174443-mkdia9zb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb' target=\"_blank\">VanillaTransoformer_7</a></strong> to <a href='https://wandb.ai/sifalklioui/EN2KAB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sifalklioui/EN2KAB' target=\"_blank\">https://wandb.ai/sifalklioui/EN2KAB</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb' target=\"_blank\">https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb</a>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b88a85b67a0>"},"metadata":{}}]},{"cell_type":"code","source":"class kabeng():\n    def __init__(self,config,src_tokenizer,tgt_tokenizer, part):\n        self.config = config\n        self.part = part\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        if part in ('train','test'):\n            df = pd.read_csv(config.data_folder+'eng2kab.tsv',sep='\\t',names=['id1','en','id2','kab'], header=None).drop(columns=['id1','id2'])\n            if part == 'train':\n                df = df[:9*len(df)//10]\n            else:\n                df = df[9*len(df)//10:]\n            self.data = Dataset.from_pandas(df ,split=self.part)\n        else:\n            raise ValueError(\"Invalid value for part, please choose train or test\")\n        self.dataset_scr,self.dataset_tgt = self.tokenize()\n\n        # create funtion to tokenize data\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx):\n\n        source_ids = self.dataset_scr[\"input_ids\"][idx].squeeze()\n        src_padding_mask  = ~self.dataset_scr[\"attention_mask\"][idx].squeeze().type(torch.bool)\n\n        target_ids = self.dataset_tgt[\"input_ids\"][idx].squeeze()\n        tgt_padding_mask = ~self.dataset_tgt[\"attention_mask\"][idx].squeeze()[:-1].type(torch.bool)\n\n\n        return {\"source_ids\": source_ids,\n                \"src_padding_mask\" : src_padding_mask,\n                \"target_ids\": target_ids,\n                \"tgt_padding_mask\": tgt_padding_mask}\n\n\n    def tokenize(self):\n\n        tokenizer_params = {\n            \"src\": {\n                \"max_length\": self.config.src_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            },\n            \"tgt\": {\n                \"max_length\": self.config.tgt_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            }\n        }\n        dataset_scr =  self.src_tokenizer(self.data['en'], **tokenizer_params[\"src\"])\n        dataset_tgt = self.tgt_tokenizer(self.data['kab'], **tokenizer_params[\"tgt\"])\n        return dataset_scr,dataset_tgt\n\ndef get_dataset(config, src_tokenizer, tgt_tokenizer,part):\n    return kabeng(config,src_tokenizer,tgt_tokenizer,part)","metadata":{"id":"YJMz1Lyi-u6g","execution":{"iopub.status.busy":"2023-10-08T17:42:27.969017Z","iopub.execute_input":"2023-10-08T17:42:27.971412Z","iopub.status.idle":"2023-10-08T17:42:28.107290Z","shell.execute_reply.started":"2023-10-08T17:42:27.971376Z","shell.execute_reply":"2023-10-08T17:42:28.106316Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout,\n                                       batch_first=True)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"id":"BevNVfbWAUx9","execution":{"iopub.status.busy":"2023-10-08T17:42:28.112397Z","iopub.execute_input":"2023-10-08T17:42:28.115268Z","iopub.status.idle":"2023-10-08T17:42:28.254365Z","shell.execute_reply.started":"2023-10-08T17:42:28.115234Z","shell.execute_reply":"2023-10-08T17:42:28.253389Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transformer = Seq2SeqTransformer(config.num_encoder_layers, config.num_decoder_layers, config.emb_size,\n                                 config.nhead, config.source_vocab_size, config.target_vocab_size, config.ffn_hid_dim)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n        \nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    transformer = nn.DataParallel(transformer)\n\ntransformer.to(device)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index= target_tokenizer.pad_token_id,label_smoothing = config.label_smoothing)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n\nscheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config.warmup_epochs, max_epochs=config.num_train_epochs)\n\n","metadata":{"id":"fg2lTvqRZHlW","execution":{"iopub.status.busy":"2023-10-08T17:45:17.990089Z","iopub.execute_input":"2023-10-08T17:45:17.992153Z","iopub.status.idle":"2023-10-08T17:45:24.670992Z","shell.execute_reply.started":"2023-10-08T17:45:17.992120Z","shell.execute_reply":"2023-10-08T17:45:24.670043Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_6125/679446711.py:18: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config.warmup_epochs, max_epochs=config.num_train_epochs)\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize the datasets\ntrain = get_dataset(config,source_tokenizer,target_tokenizer,'train')\ntest = get_dataset(config,source_tokenizer,target_tokenizer,'test')","metadata":{"execution":{"iopub.status.busy":"2023-10-08T17:45:24.672369Z","iopub.execute_input":"2023-10-08T17:45:24.672929Z","iopub.status.idle":"2023-10-08T17:45:35.728450Z","shell.execute_reply.started":"2023-10-08T17:45:24.672896Z","shell.execute_reply":"2023-10-08T17:45:35.727466Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"src_mask = torch.zeros((config.src_max_length, config.src_max_length), dtype=torch.bool).repeat(torch.cuda.device_count(), 1)\ntgt_mask = torch.tril(torch.full((config.tgt_max_length-1, config.tgt_max_length-1), float('-inf')), diagonal=-1).transpose(0, 1).repeat(torch.cuda.device_count(), 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T17:46:42.122880Z","iopub.execute_input":"2023-10-08T17:46:42.123437Z","iopub.status.idle":"2023-10-08T17:46:42.210333Z","shell.execute_reply.started":"2023-10-08T17:46:42.123397Z","shell.execute_reply":"2023-10-08T17:46:42.209220Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train, batch_size=config.train_batch_size)\nval_dataloader = DataLoader(test, batch_size=config.eval_batch_size)\n\ndef train_epoch(model, optimizer):\n    model.train()\n    losses = 0\n\n    for batch in tqdm(train_dataloader,desc='train'):\n        \n        src = batch['source_ids'].to(device)\n        tgt = batch['target_ids'].to(device)\n        src_padding_mask = batch['src_padding_mask'].to(device)\n        tgt_padding_mask = batch['tgt_padding_mask'].to(device)\n        \n        tgt_input = tgt[:, :-1]\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optimizer.zero_grad()\n        tgt_out = tgt[:, 1:]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(train_dataloader))\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n\n    for batch in tqdm(val_dataloader,desc='evaluation'):\n\n        src = batch['source_ids'].to(device)\n        tgt = batch['target_ids'].to(device)\n        src_padding_mask = batch['src_padding_mask'].to(device)\n        tgt_padding_mask = batch['tgt_padding_mask'].to(device)\n\n        tgt_input = tgt[:, :-1]\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[:, 1:]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(list(val_dataloader))","metadata":{"id":"ZVX5dG2qaZY7","execution":{"iopub.status.busy":"2023-10-08T17:46:45.436914Z","iopub.execute_input":"2023-10-08T17:46:45.437343Z","iopub.status.idle":"2023-10-08T17:46:45.527957Z","shell.execute_reply.started":"2023-10-08T17:46:45.437314Z","shell.execute_reply":"2023-10-08T17:46:45.526831Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with open(f'model_config_run_{run_num}.yaml', 'r') as yaml_file:\n        loaded_model_params = yaml.safe_load(yaml_file)\n\n# Create a new instance of the model with the loaded configuration\nmodel = Seq2SeqTransformer(\n    loaded_model_params[\"num_encoder_layers\"],\n    loaded_model_params[\"num_decoder_layers\"],\n    loaded_model_params[\"emb_size\"],\n    loaded_model_params[\"nhead\"],\n    loaded_model_params[\"source_vocab_size\"],\n    loaded_model_params[\"target_vocab_size\"],\n    loaded_model_params[\"ffn_hid_dim\"]\n)\ncheckpoint = torch.load('/kaggle/working/model_checkpoints/best_checkpoint.pt')\nnew_state_dict = {}\n\nweights = checkpoint['model_state_dict']\nfor key, value in weights.items():\n    if key.startswith('module.'):\n        new_key = key[7:]  # Remove the \"module.\" prefix\n        new_state_dict[new_key] = value\n    else:\n        new_state_dict[key] = value\nmodel.load_state_dict(new_state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_config()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T17:46:52.420009Z","iopub.execute_input":"2023-10-08T17:46:52.420663Z","iopub.status.idle":"2023-10-08T17:46:52.549083Z","shell.execute_reply.started":"2023-10-08T17:46:52.420633Z","shell.execute_reply":"2023-10-08T17:46:52.548094Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model_checkpoint_dir = 'model_checkpoints'\nos.makedirs(model_checkpoint_dir, exist_ok=True)\n\nconfig_checkpoint_dir = 'configs'\nos.makedirs(config_checkpoint_dir, exist_ok=True)\ndef save_config(run_num=run_num):\n    # Create a dictionary to store the parameters\n    model_params = {\n        \"num_encoder_layers\": config.num_encoder_layers,\n        \"num_decoder_layers\": config.num_decoder_layers,\n        \"emb_size\": config.emb_size,\n        \"nhead\": config.nhead,\n        \"source_vocab_size\": config.source_vocab_size,\n        \"target_vocab_size\": config.target_vocab_size,\n        \"ffn_hid_dim\": config.ffn_hid_dim\n    }\n\n    # Save the parameters to a YAML file (e.g., 'model_config.yaml')\n    with open(f'{config_checkpoint_dir}/model_config_run_{run_num}.yaml', 'w') as yaml_file:\n        yaml.dump(model_params, yaml_file)\n\n    \ndef load_checkpoint(model_checkpoint_dir, model,run, optimizer=None):\n    \n    with open(f'model_config_run_{run_num}.yaml', 'r') as yaml_file:\n        loaded_model_params = yaml.safe_load(yaml_file)\n\n    # Create a new instance of the model with the loaded configuration\n    loaded_transformer = Seq2SeqTransformer(\n        loaded_model_params[\"num_encoder_layers\"],\n        loaded_model_params[\"num_decoder_layers\"],\n        loaded_model_params[\"emb_size\"],\n        loaded_model_params[\"nhead\"],\n        loaded_model_params[\"source_vocab_size\"],\n        loaded_model_params[\"target_vocab_size\"],\n        loaded_model_params[\"ffn_hid_dim\"]\n    )\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    \n    return model, optimizer, epoch, loss","metadata":{"execution":{"iopub.status.busy":"2023-10-08T19:39:36.123135Z","iopub.execute_input":"2023-10-08T19:39:36.123479Z","iopub.status.idle":"2023-10-08T19:39:36.212513Z","shell.execute_reply.started":"2023-10-08T19:39:36.123452Z","shell.execute_reply":"2023-10-08T19:39:36.211455Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"best_val_loss = float('inf')  # Initialize with a large value\ncurrent_patience = 0  # Initialize patience counter\nthreshold = 0.1  # Set your desired threshold for loss improvement\ncheckpoint_patience = 8\ndef save_checkpoint(epoch, model, optimizer, val_loss = float('inf'), force = False):\n    global best_val_loss, current_patience\n    if force | (val_loss < (best_val_loss - threshold)):\n        best_val_loss = val_loss\n        current_patience = 0  # Reset patience counter\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_loss': best_val_loss\n        }\n        model_checkpoint_path = os.path.join(model_checkpoint_dir, f'best_checkpoint_run_{run_num}.pt')\n        torch.save(checkpoint, model_checkpoint_path)\n        print(f\"Checkpoint saved at {model_checkpoint_path}\")\n    else:\n        current_patience += 1\n\n    if current_patience >= checkpoint_patience:\n        print(f\"Validation loss hasn't improved for {current_patience} epochs. Stopping training.\")\n        exit()\n    else:\n        return False\n","metadata":{"execution":{"iopub.status.busy":"2023-10-08T19:39:40.054790Z","iopub.execute_input":"2023-10-08T19:39:40.055161Z","iopub.status.idle":"2023-10-08T19:39:40.139590Z","shell.execute_reply.started":"2023-10-08T19:39:40.055130Z","shell.execute_reply":"2023-10-08T19:39:40.138641Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T19:28:10.326824Z","iopub.execute_input":"2023-10-08T19:28:10.327188Z","iopub.status.idle":"2023-10-08T19:28:15.056223Z","shell.execute_reply.started":"2023-10-08T19:28:10.327159Z","shell.execute_reply":"2023-10-08T19:28:15.055383Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7cda47beff74427bcea571feb49e603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>▂▃▃▄▅▆▆▇█████▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>30</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>3.43127</td></tr><tr><td>val_loss</td><td>4.25901</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">VanillaTransoformer_7</strong> at: <a href='https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb' target=\"_blank\">https://wandb.ai/sifalklioui/EN2KAB/runs/mkdia9zb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231008_174443-mkdia9zb/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"for epoch in range(1, config.num_train_epochs+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    scheduler.step()\n    if epoch > config.warmup_epochs:\n        save_checkpoint(epoch, transformer, optimizer, val_loss)\n    wandb.log({\"Epoch\": epoch, \"train_loss\": train_loss,\"val_loss\":val_loss, \"lr\" : optimizer.param_groups[0]['lr']})\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n    \nwandb.finish()","metadata":{"id":"Km138ytlabCM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e03cc18-bb26-4e66-95b6-3aaa2d4be910","execution":{"iopub.status.busy":"2023-10-08T17:46:59.585367Z","iopub.execute_input":"2023-10-08T17:46:59.585690Z","iopub.status.idle":"2023-10-08T19:20:56.639105Z","shell.execute_reply.started":"2023-10-08T17:46:59.585664Z","shell.execute_reply":"2023-10-08T19:20:56.638058Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"train:   0%|          | 0/280 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\ntrain: 100%|██████████| 280/280 [02:59<00:00,  1.56it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Train loss: 10.312, Val loss: 10.313, Epoch time = 182.393s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:08<00:00,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Train loss: 8.369, Val loss: 7.064, Epoch time = 179.384s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Train loss: 6.564, Val loss: 6.153, Epoch time = 179.363s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:57<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Train loss: 6.000, Val loss: 5.732, Epoch time = 180.096s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Train loss: 5.635, Val loss: 5.445, Epoch time = 179.312s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Train loss: 5.381, Val loss: 5.228, Epoch time = 179.028s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Train loss: 5.149, Val loss: 5.051, Epoch time = 179.419s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Train loss: 4.956, Val loss: 4.920, Epoch time = 179.422s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Train loss: 4.788, Val loss: 4.811, Epoch time = 179.146s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Train loss: 4.659, Val loss: 4.733, Epoch time = 179.251s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at configs/best_checkpoint_run_7.pt\nEpoch: 11, Train loss: 4.535, Val loss: 4.673, Epoch time = 178.965s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Train loss: 4.428, Val loss: 4.634, Epoch time = 179.535s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Train loss: 4.338, Val loss: 4.591, Epoch time = 179.555s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:58<00:00,  1.57it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at configs/best_checkpoint_run_7.pt\nEpoch: 14, Train loss: 4.248, Val loss: 4.544, Epoch time = 180.729s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:58<00:00,  1.57it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Train loss: 4.160, Val loss: 4.508, Epoch time = 181.315s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:57<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Train loss: 4.064, Val loss: 4.482, Epoch time = 179.948s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Train loss: 3.992, Val loss: 4.447, Epoch time = 179.348s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at configs/best_checkpoint_run_7.pt\nEpoch: 18, Train loss: 3.903, Val loss: 4.417, Epoch time = 178.863s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, Train loss: 3.826, Val loss: 4.406, Epoch time = 178.970s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20, Train loss: 3.757, Val loss: 4.376, Epoch time = 179.161s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 21, Train loss: 3.695, Val loss: 4.344, Epoch time = 179.405s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 22, Train loss: 3.640, Val loss: 4.323, Epoch time = 179.388s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at configs/best_checkpoint_run_7.pt\nEpoch: 23, Train loss: 3.587, Val loss: 4.304, Epoch time = 179.088s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 24, Train loss: 3.544, Val loss: 4.293, Epoch time = 179.157s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 25, Train loss: 3.503, Val loss: 4.287, Epoch time = 179.642s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 26, Train loss: 3.472, Val loss: 4.278, Epoch time = 178.972s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 27, Train loss: 3.451, Val loss: 4.265, Epoch time = 179.114s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 28, Train loss: 3.440, Val loss: 4.258, Epoch time = 178.734s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.58it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 29, Train loss: 3.434, Val loss: 4.258, Epoch time = 179.402s\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s]\nevaluation: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 30, Train loss: 3.431, Val loss: 4.259, Epoch time = 178.905s\n","output_type":"stream"}]},{"cell_type":"code","source":"save_checkpoint(epoch,transformer.module,optimizer,force=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T19:41:28.878176Z","iopub.execute_input":"2023-10-08T19:41:28.878516Z","iopub.status.idle":"2023-10-08T19:41:32.424892Z","shell.execute_reply.started":"2023-10-08T19:41:28.878490Z","shell.execute_reply":"2023-10-08T19:41:32.423769Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Checkpoint saved at model_checkpoints/best_checkpoint_run_7.pt\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"token_config = { \"max_length\": config.src_max_length  ,\n                \"add_special_tokens\": config.add_special_tokens,\n                \"truncation\": config.truncation,\n                \"return_tensors\": config.return_tensors,\n             }","metadata":{"id":"tW3JOl1dGJGS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate an output sequence using the greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    # Move inputs to the device\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n\n    # Encode the source sequence\n    memory = model.encode(src, src_mask)\n\n    # Initialize the target sequence with the start symbol\n    ys = torch.tensor([[start_symbol]]).type(torch.long).to(device)\n\n    for i in range(max_len - 1):\n        memory = memory.to(device)\n\n        # Create a target mask for autoregressive decoding\n        tgt_mask = torch.tril(torch.full((ys.size(1), ys.size(1)), float('-inf'), device=device), diagonal=-1).transpose(0, 1).to(device)\n        # Decode the target sequence\n        out = model.decode(ys, memory, tgt_mask)\n        # Generate the probability distribution over the vocabulary\n        prob = model.generator(out[:, -1])\n\n        # Select the next word with the highest probability\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n\n        # Append the next word to the target sequence\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n\n        # Check if the generated word is the end-of-sequence token\n        if next_word == target_tokenizer.eos_token_id:\n            break\n\n    return ys\n\n# Actual function to translate input sentence into the target language\ndef translate(model: torch.nn.Module , src_sentence: str, raw: bool = False):\n    model.to(device)\n    model.eval()\n    # Tokenize the source sentence\n    src = source_tokenizer(src_sentence, **token_config)['input_ids']\n    num_tokens = src.shape[1]\n    # Create a source mask\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n\n    # Generate the target tokens using greedy decoding\n    tgt_tokens = greedy_decode(\n        model, src, src_mask, max_len=num_tokens + 5, start_symbol=target_tokenizer.bos_token_id).flatten()\n    \n    # Decode the target tokens and clean up the result\n    if raw:\n        return tgt_tokens.tolist()\n    return target_tokenizer.decode(tgt_tokens, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Greedy decoding\nresult = translate(model, \"Hello, how are you doing?\",raw=True)\ntarget_tokenizer.convert_ids_to_tokens(result,skip_special_tokens=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQIP0GRioHO1","outputId":"eaa901e9-108b-4fb3-deda-f3014c781d69","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text= 'D acu i telliḍ txeddmeḍ, neɣ ala?'\ntarget_tokenizer.tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam Search implementation\ndef beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size=5):\n    # Move inputs to the device\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n    # Encode the source sequence\n    memory = model.encode(src, src_mask)\n\n    # Initialize the beams\n    beams = [(torch.tensor([[start_symbol]]).type(torch.long).to(device), 0)]\n\n    for i in range(max_len - 1):\n        new_beams = []\n\n        for ys, score in beams:\n            memory = memory.to(device)\n\n            # Create a target mask for autoregressive decoding\n            tgt_mask = torch.tril(torch.full((ys.size(1), ys.size(1)), float('-inf'), device=device), diagonal=-1).transpose(0, 1).to(device)\n            # Decode the target sequence\n            out = model.decode(ys, memory, tgt_mask)\n            # Generate the probability distribution over the vocabulary\n            prob = model.generator(out[:, -1])\n\n            # Get the top beam_size candidates for the next word\n            _, top_indices = torch.topk(prob, beam_size, dim=1)\n\n            for next_word in top_indices[0]:\n                next_word = next_word.item()\n\n                # Append the next word to the target sequence\n                new_ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n                new_score = score + prob[0][next_word].item()\n\n                # Check if the generated word is the end-of-sequence token\n                if next_word == target_tokenizer.eos_token_id:\n                    new_beams.append((new_ys, new_score))\n                else:\n                    new_beams.append((new_ys, new_score))\n\n        # Sort the beams by score and select the top beam_size beams\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_size]\n\n    # Return the best beam\n    best_beam = beams[0][0]\n    return best_beam\n\n# Actual function to translate input sentence into the target language using beam search\ndef translate_with_beam_search(model: torch.nn.Module, src_sentence: str, beam_size=5, raw: bool = False):\n    model.to(device)\n    model.eval()\n    # Tokenize the source sentence\n    src = source_tokenizer(src_sentence, **token_config)['input_ids']\n    num_tokens = src.shape[1]\n    # Create a source mask\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n\n    # Generate the target tokens using beam search decoding\n    tgt_tokens = beam_search_decode(\n        model, src, src_mask, max_len=num_tokens + 5, start_symbol=target_tokenizer.bos_token_id, beam_size=beam_size).flatten()\n    if raw:\n        return tgt_tokens\n    # Decode the target tokens and clean up the result\n    return target_tokenizer.decode(tgt_tokens, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n","metadata":{"id":"rsLPXUP6AiFp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(translate_with_beam_search(model, \"hello, how are you?\",beam_size=5).replace(' - ','-'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"om8Vk7SdAkbz","outputId":"9d3128e6-88b2-4fcc-e59c-d2cf50747305","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It takes long to compute the BLEU Score\n\ndef bleu_score(model, src,tgt):\n    # Get the bleu score of a model\n    actual, predicted = [], []\n    for source,target in zip(src,tgt):\n        # translate encoded source text\n        translation = translate(model,source,raw=True)\n        translation = target_tokenizer.convert_ids_to_tokens(translation,skip_special_tokens=True)\n        target = target_tokenizer.tokenize(target)\n        actual.append(target)\n        predicted.append(translation)\n        \n        \n    bleu_dic = {}\n    bleu_dic['1-grams'] = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n    bleu_dic['1-2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n    bleu_dic['1-3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n    bleu_dic['1-4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return bleu_dic\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e,k = df['en'][:71541],df['kab'][:71541],\nbleu_train = bleu_score(transformer, src= e, tgt=k)\nplt.bar(x = bleu_train.keys(), height = bleu_train.values())\nplt.title(\"BLEU Score with the training set\")\nplt.ylim((0,1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e2,k2 = df['en'][79090:],df['kab'][79090:]\nbleu_test = bleu_score(model, src= e2, tgt=k2)\nplt.bar(x = bleu_test.keys(), height = bleu_test.values())\nplt.title(\"BLEU Score with the test set\")\nplt.ylim((0,1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e2,k2 = df['en'][79090:],df['kab'][79390:]\nbleu_test = bleu_score(model, src= e2, tgt=k2)\nplt.bar(x = bleu_test.keys(), height = bleu_test.values())\nplt.title(\"BLEU Score with the test set\")\nplt.ylim((0,1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}