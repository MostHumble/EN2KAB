{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1a130f14e7f043358916cc067cd297a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28aa98261b6947d9a97858dcff50ff4d","IPY_MODEL_7d2a2d2f64034c21b1d3c7ecc7ba06f7","IPY_MODEL_4bed7ad51aab48d1b9803923ba1aa042"],"layout":"IPY_MODEL_380394d7c571473dbe9d258612987f5d"}},"28aa98261b6947d9a97858dcff50ff4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8064793281bc4eff8d506682f2b1f80b","placeholder":"​","style":"IPY_MODEL_4dda68733df8475bbf4294976181c799","value":"Downloading (…)okenizer_config.json: 100%"}},"7d2a2d2f64034c21b1d3c7ecc7ba06f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a58a16faf4e44ee28c783217b723d871","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e69c52fe88244839ce1d70ededd401a","value":28}},"4bed7ad51aab48d1b9803923ba1aa042":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db7e7b07eb4c46cea94e01574bdd5e08","placeholder":"​","style":"IPY_MODEL_81ac5e57b0ba429fa741b7b320664927","value":" 28.0/28.0 [00:00&lt;00:00, 1.40kB/s]"}},"380394d7c571473dbe9d258612987f5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8064793281bc4eff8d506682f2b1f80b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dda68733df8475bbf4294976181c799":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a58a16faf4e44ee28c783217b723d871":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e69c52fe88244839ce1d70ededd401a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db7e7b07eb4c46cea94e01574bdd5e08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81ac5e57b0ba429fa741b7b320664927":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32d468f9221e4514b133f493a1dc030f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b4e8c740e214363b26b0509762ff130","IPY_MODEL_17078612a6e14a93883325f96feabe55","IPY_MODEL_0629816b65b74c4f8cecf213b2e77c3b"],"layout":"IPY_MODEL_dc677cf184f547ea92f6297d949bd82a"}},"8b4e8c740e214363b26b0509762ff130":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32be718e944942fe839c3264f1e8ed38","placeholder":"​","style":"IPY_MODEL_d6f62776211f49dfa32c6842729daf7b","value":"Downloading (…)lve/main/config.json: 100%"}},"17078612a6e14a93883325f96feabe55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a779eb728cb4a6a8e989ec8c6d577f0","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1c96ea47a8c458b9f980373a0b3df98","value":570}},"0629816b65b74c4f8cecf213b2e77c3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b14ea970f514a9bbb6838faf69c81ab","placeholder":"​","style":"IPY_MODEL_5e3d9bdb866b482ba1dd6636985e62de","value":" 570/570 [00:00&lt;00:00, 27.9kB/s]"}},"dc677cf184f547ea92f6297d949bd82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32be718e944942fe839c3264f1e8ed38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f62776211f49dfa32c6842729daf7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a779eb728cb4a6a8e989ec8c6d577f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c96ea47a8c458b9f980373a0b3df98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b14ea970f514a9bbb6838faf69c81ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3d9bdb866b482ba1dd6636985e62de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33f923d8da7b48569d1a4664cdd61df8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0c89b8f705243e980a3c9525ec420eb","IPY_MODEL_429f32080efb47d1acbf6f9fc2a1141c","IPY_MODEL_34956745f3824600968f181aca0e08d9"],"layout":"IPY_MODEL_d6ffa4759e714bc4a5aa918119a8e498"}},"e0c89b8f705243e980a3c9525ec420eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_166bc6e9565a478e8ac7dd94f660e439","placeholder":"​","style":"IPY_MODEL_e1a87ab4fdd04bcb8303acb921d38d9c","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"429f32080efb47d1acbf6f9fc2a1141c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c433b7cb134e438dbf3f2f4d97dbcb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac714bc71d4249509387746467cc1ba8","value":231508}},"34956745f3824600968f181aca0e08d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49cd6e4478fa4ac7bbf8855d28d4cd67","placeholder":"​","style":"IPY_MODEL_303c1610be9e469d9680de67dcd32ea2","value":" 232k/232k [00:00&lt;00:00, 3.56MB/s]"}},"d6ffa4759e714bc4a5aa918119a8e498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"166bc6e9565a478e8ac7dd94f660e439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1a87ab4fdd04bcb8303acb921d38d9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4c433b7cb134e438dbf3f2f4d97dbcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac714bc71d4249509387746467cc1ba8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49cd6e4478fa4ac7bbf8855d28d4cd67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"303c1610be9e469d9680de67dcd32ea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"deb29c66f35141a0929327573a91ed0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8c9bc95c4ee4fe78457dec83de9d0ce","IPY_MODEL_3490fcabdbfd473f9cd4795d82bff38e","IPY_MODEL_658b56eedc3a4f3b868c105ed739ad7f"],"layout":"IPY_MODEL_09bbc5b394ea48e5b932d95eb871a47f"}},"b8c9bc95c4ee4fe78457dec83de9d0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bacf5321e774317b39e5ac5dba3928d","placeholder":"​","style":"IPY_MODEL_a8ed3621bd3e42dd93b6f52db18bd7d5","value":"Downloading (…)/main/tokenizer.json: 100%"}},"3490fcabdbfd473f9cd4795d82bff38e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c491b5082c149e68c0a3c4f04466e61","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ba46641bd0495192292aaa1788f896","value":466062}},"658b56eedc3a4f3b868c105ed739ad7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee4009f42284792aa1818d63d01c7cc","placeholder":"​","style":"IPY_MODEL_fdfd80902cc344d4b3c15c5b2f1253da","value":" 466k/466k [00:00&lt;00:00, 7.95MB/s]"}},"09bbc5b394ea48e5b932d95eb871a47f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bacf5321e774317b39e5ac5dba3928d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8ed3621bd3e42dd93b6f52db18bd7d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c491b5082c149e68c0a3c4f04466e61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ba46641bd0495192292aaa1788f896":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aee4009f42284792aa1818d63d01c7cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdfd80902cc344d4b3c15c5b2f1253da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc167b07edb640a9be332a8474b5dc52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3daa4eb779f84ef4bed84d9b2d310b66","IPY_MODEL_1b799d15042344bf865374b4634466e1","IPY_MODEL_bd7ac327bb7e4ba5b45374027530c253"],"layout":"IPY_MODEL_45a193e0e1834e60b7fd4365a4c71fad"}},"3daa4eb779f84ef4bed84d9b2d310b66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d9c103f98ab47f6b058d84e9a0dc530","placeholder":"​","style":"IPY_MODEL_88eff466c6054b2b91e38059c7a14d9f","value":"Downloading (…)okenizer_config.json: 100%"}},"1b799d15042344bf865374b4634466e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4089022912854d78858c2559bf7e7943","max":1601,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e3fe4a317194dcdaa8f79f9df8a49af","value":1601}},"bd7ac327bb7e4ba5b45374027530c253":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5d7bc6fe6ba4e5eb1b8eec5f133d908","placeholder":"​","style":"IPY_MODEL_3ba7f4ec5f604684a854d0fafb29c5e7","value":" 1.60k/1.60k [00:00&lt;00:00, 105kB/s]"}},"45a193e0e1834e60b7fd4365a4c71fad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d9c103f98ab47f6b058d84e9a0dc530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88eff466c6054b2b91e38059c7a14d9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4089022912854d78858c2559bf7e7943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e3fe4a317194dcdaa8f79f9df8a49af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5d7bc6fe6ba4e5eb1b8eec5f133d908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba7f4ec5f604684a854d0fafb29c5e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20cfeb09a8c34e5191204329979d3dcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_272927580cff4509a04cec9af2021f58","IPY_MODEL_852f648e082740eb8cbe29aea839be0e","IPY_MODEL_e875a71d27bf493e995353f84cab2af9"],"layout":"IPY_MODEL_98ba872008024a51af8449b30fa410f7"}},"272927580cff4509a04cec9af2021f58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fde3adf95de3469c88f42ddc1413e0d0","placeholder":"​","style":"IPY_MODEL_08dbefdef6b04113b1c03e46d3393110","value":"Downloading (…)/main/tokenizer.json: 100%"}},"852f648e082740eb8cbe29aea839be0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2cd216725040ec959948e83de908d3","max":1359692,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a823ad2373e4383a6836c1f083543c8","value":1359692}},"e875a71d27bf493e995353f84cab2af9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1b4e9ca4a954797b3922a71634a83d9","placeholder":"​","style":"IPY_MODEL_f7d2698d47264aeba67ceab77f9e8f2a","value":" 1.36M/1.36M [00:00&lt;00:00, 14.7MB/s]"}},"98ba872008024a51af8449b30fa410f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fde3adf95de3469c88f42ddc1413e0d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08dbefdef6b04113b1c03e46d3393110":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d2cd216725040ec959948e83de908d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a823ad2373e4383a6836c1f083543c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1b4e9ca4a954797b3922a71634a83d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7d2698d47264aeba67ceab77f9e8f2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6813eda08fdc4a9fb4decc2ec0b33f17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_671026bfa79c44768915cf4503714b2d","IPY_MODEL_1c98901d34784175b93a0185e439b779","IPY_MODEL_65e40755c23d45d9a633f36517e0eea2"],"layout":"IPY_MODEL_73eb29f4f36b49718724f3689cc6521c"}},"671026bfa79c44768915cf4503714b2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99a8485125e24c24bf978e989951d6af","placeholder":"​","style":"IPY_MODEL_b7297e2dc10043aeb500e1e7cea77fcf","value":"Downloading (…)cial_tokens_map.json: 100%"}},"1c98901d34784175b93a0185e439b779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_203761c8e0e74d349192b01a129373b1","max":173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_838124f6ecaa4e9e8b5e0b6555249e9c","value":173}},"65e40755c23d45d9a633f36517e0eea2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be7c54368cd94f74b53884cdc48d5070","placeholder":"​","style":"IPY_MODEL_8e3b0854c61b40dc9db1c6f16f9ffbb2","value":" 173/173 [00:00&lt;00:00, 13.0kB/s]"}},"73eb29f4f36b49718724f3689cc6521c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99a8485125e24c24bf978e989951d6af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7297e2dc10043aeb500e1e7cea77fcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"203761c8e0e74d349192b01a129373b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"838124f6ecaa4e9e8b5e0b6555249e9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be7c54368cd94f74b53884cdc48d5070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3b0854c61b40dc9db1c6f16f9ffbb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install tokenizers transformers datasets -q\n!pip install wandb -qU","metadata":{"id":"zE_qPYnwb2w5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install flair -q\n!pip install botocore==1.31.17 -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\nimport os\nimport pandas as pd\nfrom tokenizers import Tokenizer\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers.processors import TemplateProcessing\nfrom transformers import AutoTokenizer\nimport matplotlib.pyplot as plt\nfrom timeit import default_timer as timer\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom dataclasses import dataclass,asdict\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau,StepLR\nimport math\n#from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm \nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nimport yaml\nimport random, torch, numpy as np\n#from flair.data import Sentence\n#from flair.models import SequenceTagger\n\nuser_secrets = UserSecretsClient()\nwandb.login(key=user_secrets.get_secret(\"wbtok\"))\n# Choose the Kaggle API token JSON file that you downloaded\n#files.upload()\n\n#os.environ['TOKENIZERS_PARALLELISM'] = 'True'","metadata":{"id":"lRslFWK0b4ND","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TOKENIZERS_PARALLELISM'] = 'True'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load tagger\n#tagger = SequenceTagger.load(\"flair/pos-english-fast\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nspecial_tokens = {'unk_token':\"[UNK]\",\n                  'cls_token':\"[CLS]\",\n                  'eos_token': '[EOS]',\n                  'sep_token':\"[SEP]\",\n                  'pad_token':\"[PAD]\",\n                  'mask_token':\"[MASK]\",\n                  'bos_token':\"[BOS]\"}\n\n# Load tokenizers\nsource_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", **special_tokens)\ntarget_tokenizer = PreTrainedTokenizerFast.from_pretrained('Sifal/E2KT',token=user_secrets.get_secret(\"hftoken\"))\ndef addPreprocessing(tokenizer):\n      tokenizer._tokenizer.post_processor = TemplateProcessing(\n          single=tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n          special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id), (tokenizer.bos_token, tokenizer.bos_token_id)])\n\naddPreprocessing(source_tokenizer)\naddPreprocessing(target_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Monolingual data\npath = \"/kaggle/input/en2kab/test.xlsx\"\n#files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".txt\")]\n# Kabyle English pairs\ndf = pd.read_excel(path)","metadata":{"id":"65nY8ubScYci","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def enforce_reproducibility(use_seed=None):\n    seed = use_seed if use_seed is not None else random.randint(1, 1000000)\n    print(f\"Using seed: {seed}\")\n\n    random.seed(seed)    # python RNG\n    np.random.seed(seed) # numpy RNG\n\n    # pytorch RNGs\n    torch.manual_seed(seed)          # cpu + cuda\n    torch.cuda.manual_seed_all(seed) # multi-gpu - can be called without gpus\n    if use_seed: # slower speed! https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark     = False\n\n    return seed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## from dataclasses import dataclass\nseed_ = enforce_reproducibility()\n\n@dataclass\nclass Config:\n    seed: int = seed_\n    data_folder: str = \"/kaggle/input/en2kab/\"\n    output_dir: str = './logs'\n    src_max_length: int = 21\n    tgt_max_length: int = 22\n    add_special_tokens: bool = True\n    truncation: bool = True\n    return_tensors: str = 'pt'\n    padding: str = True\n    emb_size: int = 512\n    source_vocab_size: int = len(source_tokenizer) # Initialize to 0 or provide the actual value\n    part_of_speech_vocab_size : int = 64\n    part_of_speech_pad_id = 54\n    target_vocab_size: int = target_tokenizer.vocab_size  # Initialize to 0 or provide the actual value\n    num_encoder_layers: int = 5\n    num_decoder_layers: int = 5\n    nhead: int = 2\n    ffn_hid_dim: int = 2048\n    train_batch_size: int = 288\n    eval_batch_size: int = 256\n    learning_rate: float = 1e-4\n    warmup_start: float = 1e-4\n    scheduler: str = ''\n    num_train_epochs: int = 5\n    warmup_epochs: int = 10\n    label_smoothing : float = 0.0\n        \n\nprefix = 'APOSEM'\nrun_num = 0\nconfig = Config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the YAML content from the file\nwith open('/kaggle/working/configs/finetune_model_config_run_1.yaml', 'r') as file:\n    yaml_config = yaml.safe_load(file)\n\n# Update the `config` object with values from the YAML file if the keys are present\nfor key, value in yaml_config.items():\n    if hasattr(config, key):\n        setattr(config, key, value)\n    \n# Modify the 'scheduler' key to 'ReduceLROnPlateau'\nsetattr(config,'scheduler','StepLR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(\n      # Set the project where this run will be logged\n      project=\"EN2KAB\", \n      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n      name=f\"{prefix}_{run_num}\", \n      # Track hyperparameters and run metadata\n      config=asdict(config))","metadata":{"id":"BRHqzzd2NnT7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class kabeng():\n    def __init__(self, part, path):\n        assert part in ('train', 'test'), ValueError(\"Invalid value for part, please choose train or test\")\n        \n        df = pd.read_excel(path + part + '.xlsx')\n        self.data = df\n        self.labels = self.prepare_labels()\n    \n    def prepare_labels(self):\n        labels_list = []\n        for sentence in self.data['en']:\n            tokenized = Sentence(source_tokenizer.tokenize(sentence))\n            tagger.predict(tokenized)\n            labels = [62] + [tagger.label_dictionary.get_idx_for_item(label.value) for label in tokenized.get_labels(\"pos\")]+ [63]\n\n            # Pad or truncate the labels to the max_length using NumPy\n            labels = np.array(labels)\n\n            if labels.shape[0] < config.src_max_length:\n                labels = np.pad(labels, (0, config.src_max_length - labels.shape[0]), mode='constant', constant_values=config.part_of_speech_pad_id)\n\n            labels_list.append(labels)\n\n        return labels_list\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        labels = self.labels[idx]\n\n        return {'kab': data['kab'],\n                'en': data['en'],\n                'pos': labels}\n\n# initialize the datasets\ndef get_datasets(path):\n    return kabeng('train',path), kabeng('test', path)","metadata":{"id":"YJMz1Lyi-u6g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = get_datasets(config.data_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Load the test variable from the saved file\nwith open('test_dataset.pkl', 'rb') as file:\n    test = pickle.load(file)\nwith open('train_dataset.pkl', 'rb') as file:\n    train = pickle.load(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_params = {\"add_special_tokens\": config.add_special_tokens,\n                \"truncation\": config.truncation,\n                \"return_tensors\": config.return_tensors,\n                \"padding\": config.padding}\n\ndef collate(batch):\n    \n    en = source_tokenizer([item['en'] for item in batch] ,**tokenizer_params,max_length = config.src_max_length)\n    kab = target_tokenizer([item['kab'] for item in batch],**tokenizer_params, max_length = config.tgt_max_length)\n    \n    src_max_lenght = en['attention_mask'].size(1)\n    tgt_max_lenght = kab['attention_mask'].size(1)\n    source_ids_poem = torch.tensor(np.array([item['pos'][:src_max_lenght] for item in batch]))\n\n    source_mask = torch.zeros((src_max_lenght, src_max_lenght), dtype=torch.bool)\n    target_mask = torch.tril(torch.full((tgt_max_lenght -1, tgt_max_lenght -1), float('-inf')), diagonal=-1).transpose(0, 1)\n\n\n    \n    return {'source_ids_poem' : source_ids_poem,\n            'source_input_ids':en['input_ids'],\n            'source_padding_mask': ~en['attention_mask'].type(torch.bool),\n            'source_mask' : source_mask,\n            'target_input_ids': kab['input_ids'],\n            'target_padding_mask': ~kab['attention_mask'][:,:-1].type(torch.bool),\n            'target_mask': target_mask\n            }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor, peom_embedding: Tensor = 0):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :] + peom_embedding)\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n    \n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass POSEmbedding(nn.Module):\n    def __init__(self, part_of_speech_vocab_size: int, emb_size):\n        super(POSEmbedding, self).__init__()\n        self.embedding = nn.Embedding(part_of_speech_vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 part_of_speech_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout,\n                                       batch_first=True)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n        self.poem = POSEmbedding(part_of_speech_vocab_size, emb_size)\n\n    def forward(self,\n                src: Tensor,\n                src_poem: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src),self.poem(src_poem))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src),self.poem(src_poem)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"id":"BevNVfbWAUx9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = Seq2SeqTransformer(config.num_encoder_layers, config.num_decoder_layers, config.emb_size,\n                                 config.nhead, config.source_vocab_size, config.part_of_speech_vocab_size,\n                                 config.target_vocab_size, config.ffn_hid_dim)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n        \nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    transformer = nn.DataParallel(transformer)\n\ntransformer.to(device)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index= target_tokenizer.pad_token_id,label_smoothing = config.label_smoothing)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-8)\n\n#scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_start_lr= config.warmup_start, warmup_epochs=config.warmup_epochs, max_epochs=config.num_train_epochs)\n","metadata":{"id":"fg2lTvqRZHlW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = get_datasets(config.data_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.multiprocessing.get_logger()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train, batch_size=config.train_batch_size,num_workers=4,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(train_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train, batch_size=config.train_batch_size,collate_fn=collate,num_workers=2,shuffle=True)\nval_dataloader = DataLoader(test, batch_size=config.train_batch_size,collate_fn=collate,num_workers=2)\n\ndef train_epoch(model, optimizer):\n    model.train()\n    losses = 0\n\n    for batch in tqdm(train_dataloader,desc='train'):\n        \n        source_input_ids = batch['source_input_ids'].to(device)\n        source_padding_mask = batch['source_padding_mask'].to(device)\n                \n        source_ids_poem = torch.tensor(np.array(batch['source_ids_poem']),device=device)\n        \n        target_input_ids = batch['target_input_ids'].to(device)\n        target_padding_mask = batch['target_padding_mask'].to(device)\n\n        source_mask = batch['source_mask']\n        target_mask = batch['target_mask']\n\n        if device.type != 'cpu':\n            source_mask = source_mask.repeat(torch.cuda.device_count(), 1).to(device)\n            target_mask = target_mask.repeat(torch.cuda.device_count(), 1).to(device)\n        else:\n            source_mask = source_mask.to(device)\n            target_mask = target_mask.to(device)\n            \n        target_input_ids_ = target_input_ids[:, :-1]\n\n        logits = model(source_input_ids, source_ids_poem, target_input_ids_, source_mask, target_mask,source_padding_mask, target_padding_mask, source_padding_mask)\n    \n        optimizer.zero_grad()\n        _target_input_ids = target_input_ids[:, 1:]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), _target_input_ids.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(train_dataloader))\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    for batch in tqdm(val_dataloader,desc='evaluation'):\n\n        source_input_ids = batch['source_input_ids'].to(device)\n        source_padding_mask = batch['source_padding_mask'].to(device)\n                \n        source_ids_poem = torch.tensor(np.array(batch['source_ids_poem']),device=device)\n\n        target_input_ids = batch['target_input_ids'].to(device)\n        target_padding_mask = batch['target_padding_mask'].to(device)\n        \n        source_mask = batch['source_mask']\n        target_mask = batch['target_mask']\n\n        if device.type != 'cpu':\n            source_mask = source_mask.repeat(torch.cuda.device_count(), 1).to(device)\n            target_mask = target_mask.repeat(torch.cuda.device_count(), 1).to(device)\n        else:\n            source_mask = source_mask.to(device)\n            target_mask = target_mask.to(device)\n\n        target_input_ids_ = target_input_ids[:, :-1]\n        \n        logits = model(source_input_ids, source_ids_poem, target_input_ids_, source_mask, target_mask,source_padding_mask, target_padding_mask, source_padding_mask)\n\n        optimizer.zero_grad()\n        _target_input_ids = target_input_ids[:, 1:]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), _target_input_ids.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(val_dataloader))","metadata":{"id":"ZVX5dG2qaZY7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint_dir = 'model_checkpoints'\nos.makedirs(model_checkpoint_dir, exist_ok=True)\n\nconfig_checkpoint_dir = 'configs'\nos.makedirs(config_checkpoint_dir, exist_ok=True)\ndef save_config(runum= 0,prefix:str =''):\n    # Create a dictionary to store the parameters\n    model_params = asdict(config)\n\n    # Save the parameters to a YAML file (e.g., 'model_config.yaml')\n    with open(f'{config_checkpoint_dir}/{prefix}_model_config_run_{run_num}.yaml', 'w') as yaml_file:\n        yaml.dump(model_params, yaml_file)\n\n    \ndef load_checkpoint(run, model_checkpoint_dir='/kaggle/working/model_checkpoints',config_dir='/kaggle/working/configs', return_optimizer_state:bool = False, prefix:str = ''):\n    \n    with open(f'{config_dir}/{prefix}_model_config_run_{run}.yaml', 'r') as yaml_file:\n        loaded_model_params = yaml.safe_load(yaml_file)\n\n    # Create a new instance of the model with the loaded configuration\n    loaded_transformer = Seq2SeqTransformer(\n        loaded_model_params[\"num_encoder_layers\"],\n        loaded_model_params[\"num_decoder_layers\"],\n        loaded_model_params[\"emb_size\"],\n        loaded_model_params[\"nhead\"],\n        loaded_model_params[\"source_vocab_size\"],\n        loaded_model_params[\"target_vocab_size\"],\n        loaded_model_params[\"ffn_hid_dim\"]\n    )\n    print(loaded_model_params)\n    if torch.cuda.is_available():\n        checkpoint = torch.load(f'{model_checkpoint_dir}/{prefix}_best_checkpoint_run_{run}.pt')\n    else:\n        checkpoint = torch.load(f'{model_checkpoint_dir}/{prefix}_best_checkpoint_run_{run}.pt',map_location=torch.device('cpu'))\n    \n        # Remove the \"module.\" prefix from parameter names caused by trained with ddp\n    new_state_dict = {}\n    for key, value in checkpoint['model_state_dict'].items():\n        if key.startswith('module.'):\n            new_key = key[7:]  # Remove the \"module.\" prefix\n            new_state_dict[new_key] = value\n        else:\n            new_state_dict[key] = value\n            \n    epoch = checkpoint['epoch']\n    loss = checkpoint['best_val_loss']\n    loaded_transformer.load_state_dict(new_state_dict)\n    if return_optimizer_state:\n        return  loaded_transformer, checkpoint['optimizer_state_dict'], epoch, loss\n    return loaded_transformer, None, epoch, loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_config(run_num,prefix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = float('inf')  # Initialize with a large value\ncurrent_patience = 0  # Initialize patience counter\nthreshold = 0.01  # Set your desired threshold for loss improvement\ncheckpoint_patience = 8\n\ndef save_checkpoint(epoch, model, optimizer, val_loss = float('inf'), force = False, prefix:str = ''):\n    global best_val_loss, current_patience\n    if force | (val_loss < (best_val_loss - threshold)):\n        best_val_loss = val_loss\n        current_patience = 0  # Reset patience counter\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_loss': best_val_loss\n        }\n        model_checkpoint_path = os.path.join(model_checkpoint_dir, f'{prefix}_best_checkpoint_run_{run_num}.pt')\n        torch.save(checkpoint, model_checkpoint_path)\n        print(f\"Checkpoint saved at {model_checkpoint_path}\")\n    else:\n        current_patience += 1\n\n    if current_patience >= checkpoint_patience:\n        current_patience\n        print(f\"Validation loss hasn't improved for {current_patience} epochs. Stopping training.\")\n    else:\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(6, 31):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    #scheduler.step()\n    if epoch > config.warmup_epochs:\n        save_checkpoint(epoch, transformer, optimizer, val_loss,pefix=prefix)\n    wandb.log({\"Epoch\": epoch, \"train_loss\": train_loss,\"val_loss\":val_loss, \"lr\" : optimizer.param_groups[0]['lr']})\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))","metadata":{"id":"Km138ytlabCM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e03cc18-bb26-4e66-95b6-3aaa2d4be910","trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"evaluation:  32%|███▏      | 9/28 [00:04<00:09,  1.91it/s]","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetune part","metadata":{}},{"cell_type":"code","source":"save_config(runum= 2, prefix='finetune_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model,optimizer_state,_,__ = load_checkpoint(run=2,return_optimizer_state = True,prefix='finetune_')\n\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-8)\noptimizer.load_state_dict(optimizer_state)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index= target_tokenizer.pad_token_id,label_smoothing = config.label_smoothing)\nscheduler = StepLR(optimizer, step_size=3, gamma=0.8, last_epoch=-1, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, 15):\n    start_time = timer()\n    train_loss = train_epoch(model, optimizer)\n    end_time = timer()\n    val_loss = evaluate(model)\n    scheduler.step(val_loss)\n    if epoch > 4:\n        save_checkpoint(epoch, model, optimizer, val_loss, prefix= 'finetune_')\n    wandb.log({\"Epoch\": epoch, \"train_loss\": train_loss,\"val_loss\":val_loss, \"lr\" : optimizer.param_groups[0]['lr']})\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_checkpoint(epoch, model, optimizer, val_loss, prefix= 'finetune_',force=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save_checkpoint(epoch,transformer.module,optimizer,val_loss,force=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_config = {\n                \"add_special_tokens\": config.add_special_tokens,\n                \"return_tensors\": config.return_tensors,\n             }","metadata":{"id":"tW3JOl1dGJGS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate an output sequence using the greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    # Move inputs to the device\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n    # Encode the source sequence\n    memory = model.encode(src, src_mask)\n\n    # Initialize the target sequence with the start symbol\n    ys = torch.tensor([[start_symbol]]).type(torch.long).to(device)\n\n    for i in range(max_len - 1):\n        memory = memory.to(device)\n        # Create a target mask for autoregressive decoding\n        tgt_mask = torch.tril(torch.full((ys.size(1), ys.size(1)), float('-inf'), device=device), diagonal=-1).transpose(0, 1).to(device)\n        # Decode the target sequence\n        out = model.decode(ys, memory, tgt_mask)\n        # Generate the probability distribution over the vocabulary\n        prob = model.generator(out[:, -1])\n\n        # Select the next word with the highest probability\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n\n        # Append the next word to the target sequence\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n\n        # Check if the generated word is the end-of-sequence token\n        if next_word == target_tokenizer.eos_token_id:\n            break\n\n    return ys\n\n# Actual function to translate input sentence into the target language\ndef translate(model: torch.nn.Module , src_sentence: str, raw: bool = False):\n    model.to(device)\n    model.eval()\n    # Tokenize the source sentence\n    src = source_tokenizer(src_sentence, **token_config)['input_ids']\n    num_tokens = src.shape[1]\n    # Create a source mask\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n\n    # Generate the target tokens using greedy decoding\n    tgt_tokens = greedy_decode(\n        model, src, src_mask, max_len=num_tokens + 5, start_symbol=target_tokenizer.bos_token_id).flatten()\n    \n    # Decode the target tokens and clean up the result\n    if raw:\n        return tgt_tokens.tolist()\n    return target_tokenizer.decode(tgt_tokens, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam Search implementation\ndef beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size ,length_penalty):\n    # Move inputs to the device\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n    # Encode the source sequence\n    memory = model.encode(src, src_mask) # b * seqlen_src * hdim\n\n    # Initialize the beams (sequences, score)\n    beams = [(torch.tensor([[start_symbol]]).type(torch.long).to(device), 0)] \n\n    for i in range(max_len - 1):\n        new_beams = []\n        complete_beams = []\n        cbl = []\n\n        for ys, score in beams:\n\n            # Create a target mask for autoregressive decoding\n            tgt_mask = torch.tril(torch.full((ys.size(1), ys.size(1)), float('-inf'), device=device), diagonal=-1).transpose(0, 1).to(device)\n            # Decode the target sequence\n            out = model.decode(ys, memory, tgt_mask) # b * seqlen_tgt * hdim\n            #print(f'shape out {out.shape}')\n            # Generate the probability distribution over the vocabulary\n            prob = model.generator(out[:, -1]) # b * tgt_vocab_size\n            #print(f'shape prob {prob.shape}')\n\n            # Get the top beam_size candidates for the next word\n            _, top_indices = torch.topk(prob, beam_size, dim=1) # b * beam_size\n            \n            for j,next_word in enumerate(top_indices[0]):\n\n                next_word = next_word.item()\n                \n                # Append the next word to the target sequence\n                new_ys = torch.cat([ys, torch.full((1, 1), fill_value=next_word, dtype=src.dtype).to(device)], dim=1)\n                \n                length_factor = (5 + j / 6) ** length_penalty\n                new_score = (score + prob[0][next_word].item()) / length_factor\n                \n                if next_word == target_tokenizer.eos_token_id:\n                    complete_beams.append((new_ys, new_score))\n                else:\n                    new_beams.append((new_ys, new_score))\n            \n        \n        # Sort the beams by score and select the top beam_size beams\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        try:\n            beams = new_beams[:beam_size]\n        except:\n            beams = new_beams\n\n    beams = new_beams + complete_beams\n    beams.sort(key=lambda x: x[1], reverse=True)\n    \n    best_beam = beams[0][0]\n    return best_beam\n\n\n# Actual function to translate input sentence into the target language using beam search\ndef translate_with_beam_search(model: torch.nn.Module, src_sentence: str, lenght_extend :int = 0, beam_size: int = 5, raw: bool = False, length_penalty:float = 0.6):\n    model.to(device)\n    model.eval()\n    # Tokenize the source sentence\n    src = source_tokenizer(src_sentence, **token_config)['input_ids']\n    num_tokens = src.shape[1]\n    # Create a source mask\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n\n    # Generate the target tokens using beam search decoding\n    tgt_tokens = beam_search_decode(\n        model, src, src_mask, max_len=num_tokens + lenght_extend, start_symbol=target_tokenizer.bos_token_id, beam_size=beam_size,length_penalty=length_penalty).flatten()\n    if raw:\n        return tgt_tokens\n    # Decode the target tokens and clean up the result\n    return target_tokenizer.decode(tgt_tokens, clean_up_tokenization_spaces=False, skip_special_tokens=False)\n","metadata":{"id":"rsLPXUP6AiFp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model,___,_,__ = load_checkpoint(run=2,return_optimizer_state = False,prefix='finetune_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"story = '''Once upon a time, in a quaint village nestled between rolling hills, there lived a wise old oak tree named Oliver. Oliver had witnessed countless seasons, whispered secrets to the wind, and offered shade to generations of villagers.'''.split('.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(model,\"I am still looking for a better way.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://Sifal: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@huggingface.co/spaces/Sifal/En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/En2Kab/model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://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:Sifal@huggingface.co/spaces/Sifal/En2Kab\ntorch.save(model.state_dict(), '/kaggle/working/En2Kab/model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email None\n!git config --global user.name None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git remote add origin https://Sifal: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@huggingface.co/spaces/Sifal/En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"sifalklioui@yahoo.com\"\n!git config --global user.name \"MostHumble\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add .\n!git commit -m 'add model'\n!git push origin ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_tokenizer.tokenize('tisegnit')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate_with_beam_search(model,'I am still looking for a better way to build this.',beam_size=9,lenght_extend=5,length_penalty=0.6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://huggingface.co/spaces/Sifal/En2Kab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for s in story:\n    print('sentence: ',s,'\\ntokenization: ', source_tokenizer.tokenize(s))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"om8Vk7SdAkbz","outputId":"9d3128e6-88b2-4fcc-e59c-d2cf50747305","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the range for the random index\nmin_idx,max_idx = 71541, len(df)\n\n# Generate a random index within the specified range\nrandom_idx = random.randint(min_idx, max_idx - 1)\n\n# Select the English sentence at the random index\nsampled_sentence = df['en'].iloc[random_idx]\n\n# Translate the sampled English sentence to Kabyle using beam search\ntranslated_sentence = translate(model, sampled_sentence)\n\n# Get the equivalent target sentence in Kabyle\ntarget_sentence = df['kab'].iloc[random_idx]\n\n# Print the results\nprint(\"Random English Sentence:\", sampled_sentence)\nprint(\"Translated Kabyle Sentence:\", translated_sentence)\nprint(\"Equivalent Target Kabyle Sentence:\", target_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the range for the random index\nmin_idx = 71541\nmax_idx = len(df)\n\n# Generate a random index within the specified range\nrandom_idx = random.randint(min_idx, max_idx - 1)\n\n# Select the English sentence at the random index\nsampled_sentence = df['en'].iloc[random_idx]\n\n# Translate the sampled English sentence to Kabyle using beam search\ntranslated_sentence = translate_with_beam_search(model, sampled_sentence, beam_size=5, lenght_extend=3)\n\n# Get the equivalent target sentence in Kabyle\ntarget_sentence = df['kab'].iloc[random_idx]\n\n# Print the results\nprint(\"Random English Sentence:\", sampled_sentence)\nprint(\"Translated Kabyle Sentence:\", translated_sentence)\nprint(\"Equivalent Target Kabyle Sentence:\", target_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It takes long to compute the BLEU Score\n\ndef bleu_score(model, src,tgt):\n    # Get the bleu score of a model\n    actual, predicted = [], []\n    for source,target in zip(src,tgt):\n        # translate encoded source text\n        translation = translate(model,source,raw=True)\n        translation = target_tokenizer.convert_ids_to_tokens(translation,skip_special_tokens=True)\n        target = target_tokenizer.tokenize(target)\n        actual.append(target)\n        predicted.append(translation)\n        \n    bleu = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return bleu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e,k = df['en'][:71541],df['kab'][:71541],\nbleu_train = bleu_score(model, src= e, tgt=k)\nbleu_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}